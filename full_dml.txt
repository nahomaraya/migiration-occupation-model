
================================================================================
IPUMS IMMIGRATION STUDY - XGBOOST PIPELINE
================================================================================


================================================================================
MODEL: PREDICTING OCCUPATIONAL SCORE (OCCSCORE)
================================================================================

================================================================================
DML ANALYSIS - FROM RAW FILE
================================================================================
================================================================================
STEP 0: CREATE RAM-FRIENDLY SAMPLE
================================================================================

  Source: ipums_data_ml_ready.parquet
  Total rows: 35,805,107
  Sample fraction: 10%
  âœ“ Sampled: 3,580,510 rows
  âœ“ Memory: 396.1 MB
  âœ“ Saved sample to: ipums_data_ml_ready_dml_sample.parquet

  Note: 10% sample is statistically sufficient for DML

================================================================================
DOUBLE MACHINE LEARNING PIPELINE
================================================================================

================================================================================
PREPROCESSING FOR DML
================================================================================

  Treatment: is_immigrant
  Outcome: occscore
  Excluding from features: 32 columns
================================================================================
MEMORY-SAFE XGBOOST PREPROCESSING
================================================================================

[1/6] Loading data with Polars...
  âœ“ Loaded: 3,580,510 rows Ã— 65 columns
  Memory: 396.1 MB

[2/6] Preparing train-test split indices...
  âœ“ Stratifying by: is_immigrant
  Train indices: 1,790,255 (50.0%)
  Test indices: 1,790,255 (50.0%)

[3/6] Selecting features...
  Features: 44 columns
  Excluded: 34 columns
  âœ“ No null values

[4/6] Splitting data using indices...
  âœ“ Split complete
  X_train memory: 107.6 MB
  X_test memory: 107.6 MB

[6/6] Converting to Pandas for XGBoost...
  âœ“ Conversion complete
  X_train: 112.8 MB
  X_test: 112.8 MB

[Post-Processing] Removing 2 constant columns

================================================================================
PREPROCESSING COMPLETE!
================================================================================
Final shapes:
  X_train: (1790255, 42) (109.2 MB)
  X_test:  (1790255, 42) (109.2 MB)
  Features: 42

Target distribution:
  Train: {42: 260397, 25: 202283, 27: 110201, 24: 102952, 23: 98387, 33: 94517, 20: 93799, 16: 87972, 22: 75232, 38: 56140, 11: 50656, 30: 49494, 21: 48077, 19: 47206, 18: 37498, 26: 28215, 41: 24918, 32: 24056, 13: 23153, 36: 22921, 34: 21183, 31: 20623, 14: 19525, 35: 16574, 43: 14954, 17: 14890, 39: 14448, 62: 14237, 46: 12931, 40: 12636, 45: 11972, 9: 11345, 80: 11243, 49: 8214, 15: 8153, 28: 7661, 29: 6267, 8: 4668, 37: 4308, 12: 2728, 54: 2607, 63: 2182, 10: 2160, 52: 2138, 48: 2059, 3: 1977, 4: 498}
  Test:  {42: 260863, 25: 202267, 27: 110130, 24: 103008, 23: 98199, 33: 94317, 20: 94056, 16: 87820, 22: 75092, 38: 56497, 11: 50499, 30: 49641, 21: 47625, 19: 47390, 18: 37134, 26: 28297, 41: 24949, 32: 24091, 13: 22962, 36: 22794, 34: 21026, 31: 20635, 14: 19655, 35: 16578, 17: 14857, 39: 14578, 43: 14513, 62: 14361, 40: 12818, 46: 12796, 45: 12220, 9: 11280, 80: 11077, 49: 8126, 15: 8107, 28: 7661, 29: 6335, 8: 4926, 37: 4221, 12: 2847, 54: 2676, 63: 2307, 10: 2275, 52: 2094, 48: 2090, 3: 2016, 4: 549}

  Loading treatment variable (is_immigrant)...

  Split Summary (50/50 for DML cross-fitting):
    Train: 1,790,255 samples
    Eval:  1,790,255 samples
    Treatment rate (train): 14.6%
    Treatment rate (eval):  14.6%

================================================================================
STEP 1: CLEAN THE OUTCOME (Y MODEL)
================================================================================

  Question: Can we predict OCCSCORE ignoring immigration?
  Goal: Residuals = 'Unexplained Success'

  Training XGBoost Regressor on cuda:0...

  Evaluating model with comprehensive metrics...

================================================================================
REGRESSION RESULTS
================================================================================
Metric               Train           Test
--------------------------------------------------
RMSE                 9.0751          9.2064
MSE                  82.3581         84.7575
MAE                  7.0452          7.1370
R2                   0.2985          0.2768
MAPE                 0.2969          0.3014
RESIDUAL_MEAN        0.0002          -0.0126
RESIDUAL_STD         9.0751          9.2064

  Result: Residual = Actual - Predicted
    Positive residual â†’ doing BETTER than predicted
    Negative residual â†’ doing WORSE than predicted

================================================================================
STEP 2: CLEAN THE TREATMENT (T MODEL)
================================================================================

  Question: Can we predict who is an immigrant?
  Goal: Remove selection bias â†’ 'Pure' immigration variation

  Training XGBoost Classifier on cuda:0...

  Evaluating model with comprehensive metrics...

================================================================================
CLASSIFICATION RESULTS
================================================================================
Metric               Train           Test
--------------------------------------------------
ACCURACY             0.9329          0.9288
PRECISION            0.9307          0.9262
RECALL               0.9329          0.9288
F1                   0.9305          0.9262
AUC                  0.9512          0.9438
LOGLOSS              0.1830          0.1945

Detailed Classification Report:
              precision    recall  f1-score   support

      Native       0.95      0.98      0.96   1529675
   Immigrant       0.84      0.68      0.75    260580

    accuracy                           0.94   1790255
   macro avg       0.89      0.83      0.86   1790255
weighted avg       0.93      0.94      0.93   1790255


  Result: Residual = Actual (0/1) - Probability
    This strips away selection bias
    (e.g., immigrants tend to be younger, live in cities)

================================================================================
STEP 3: CAUSAL ESTIMATION (OLS)
================================================================================

  Model: Unexplained_OCCSCORE ~ Pure_Immigration_Status
  Method: Ordinary Least Squares (statsmodels)

================================================================================
                    OLS REGRESSION RESULTS
                    (Copy-paste for thesis)
================================================================================
                            WLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.000
Model:                            WLS   Adj. R-squared:                  0.000
Method:                 Least Squares   F-statistic:                     812.2
Date:                Thu, 27 Nov 2025   Prob (F-statistic):          1.33e-178
Time:                        13:38:58   Log-Likelihood:                   -inf
No. Observations:             1790255   AIC:                               inf
Df Residuals:                 1790253   BIC:                               inf
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.0125      0.007     -1.815      0.070      -0.026       0.001
x1            -0.8405      0.030    -28.490      0.000      -0.898      -0.783
==============================================================================
Omnibus:                   339364.216   Durbin-Watson:                   2.001
Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2047988.796
Skew:                           0.780   Prob(JB):                         0.00
Kurtosis:                       8.002   Cond. No.                         4.29
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.

================================================================================
KEY FINDING: CAUSAL EFFECT OF IMMIGRATION
================================================================================

  Coefficient (Î²): -0.8405
  Standard Error:  0.0295
  t-statistic:     -28.4900
  P-value:         0.000000
  95% CI:          [-0.8983, -0.7827]
  R-squared:       0.0005

  Formatted for table: -0.8405***
                       (0.0295)

------------------------------------------------------------
INTERPRETATION:
------------------------------------------------------------

  âœ“ Statistically significant NEGATIVE effect (p < 0.05)

  Being an immigrant DECREASES occupational score by
  0.84 points, after controlling for
  education, age, region, and other confounders.

  This suggests an 'immigrant penalty' in the labor market.

=====================================================================================
AVERAGE TREATMENT EFFECT (ATE) OF IMMIGRATION ON OCCSCORE
=====================================================================================

Estimate                      Effect         SE                 95% CI      P-Value
-------------------------------------------------------------------------------------
DML (Standard SE)            -0.8405***     0.0295     [-0.8983, -0.7827]       0.0000
DML (Robust HC3 SE)          -0.8405***     0.0295     [-0.8983, -0.7827]       0.0000
-------------------------------------------------------------------------------------
Note: * p<0.05, ** p<0.01, *** p<0.001

-------------------------------------------------------------------------------------
INTERPRETATION:
-------------------------------------------------------------------------------------

  âœ“ SIGNIFICANT: Immigration DECREASES OCCSCORE by 0.84 points
    (p = 0.0000, 95% CI: [-0.90, -0.78])

âœ“ Forest plot saved to: ate_forest_plot.png
======================================================================
STEP 4: HIGH-ERROR CLUSTERING (OPTIMIZED)
======================================================================

4.1: Identifying High-Error Cases
--------------------------------------------------
  Total samples: 1,790,255
  Error threshold (top 5%): 17.43
  High-error cases: 89,513

  Sampling 50,000 from 89,513 high-error cases

4.2: High vs Low Error Comparison
--------------------------------------------------

  Metric                 High Error    Low Error       Diff
  -------------------------------------------------------
  Avg Outcome                 41.03        27.85     +13.18
  Avg |Error|                 22.75         6.37     +16.37
  Treatment %                 16.78        14.44      +2.34

  Over-predicted: 30,315 (33.9%)
  Under-predicted: 59,198 (66.1%)

4.3: Feature Selection
--------------------------------------------------
  Using top 20 features by variance
  Prepared: 50,000 samples Ã— 20 features

4.4: Finding Optimal K
--------------------------------------------------

    K   Silhouette      Inertia
  ------------------------------
    2       0.1176     881151.5
    3       0.1495     761270.4
    4       0.1376     714114.1
    5       0.1377     684574.1
    6       0.1370     646558.2
    7       0.1377     617643.9
    8       0.1327     599993.2

  â†’ Optimal K = 3

4.5: Fitting Final Model
--------------------------------------------------
  Silhouette Score: 0.1495

4.6: Cluster Profiles
======================================================================

  CLUSTER 0: OVERACHIEVERS ðŸ“ˆ
  â”œâ”€ Size: 16,061 (32.1%)
  â”œâ”€ Avg Outcome: 39.1
  â”œâ”€ Avg Residual: +5.57
  â”œâ”€ Treatment %: 15.3%
  â””â”€ Over-predicted: 42.1%
      Top features vs mean:
        â†“ degfieldd: 6.89 (avg: 1522.43)
        â†“ ancestr1: 340.93 (avg: 411.48)
        â†“ degfield: 0.07 (avg: 15.21)

  CLUSTER 1: OVERACHIEVERS ðŸ“ˆ
  â”œâ”€ Size: 15,923 (31.8%)
  â”œâ”€ Avg Outcome: 46.1
  â”œâ”€ Avg Residual: +6.02
  â”œâ”€ Treatment %: 18.6%
  â””â”€ Over-predicted: 44.0%
      Top features vs mean:
        â†‘ degfieldd: 4604.74 (avg: 1522.43)
        â†“ ancestr1: 362.63 (avg: 411.48)
        â†‘ degfield: 45.99 (avg: 15.21)

  CLUSTER 2: OVERACHIEVERS ðŸ“ˆ
  â”œâ”€ Size: 18,016 (36.0%)
  â”œâ”€ Avg Outcome: 38.4
  â”œâ”€ Avg Residual: +13.23
  â”œâ”€ Treatment %: 16.6%
  â””â”€ Over-predicted: 17.4%
      Top features vs mean:
        â†“ degfieldd: 149.28 (avg: 1522.43)
        â†‘ ancestr1: 517.55 (avg: 411.48)
        â†“ degfield: 1.49 (avg: 15.21)

--------------------------------------------------
4.7: Treatment Effects by Cluster
--------------------------------------------------

  Cluster       N     Effect       SE      p-val
  ---------------------------------------------
        0  16,061     -3.228    0.889     0.0003 ***
        1  15,923     -2.039    0.770     0.0081 **
        2  18,016     -0.820    0.491     0.0950
