# IPUMS Immigration Study - Occupational Outcomes Prediction

A machine learning pipeline using XGBoost to predict occupational outcomes for immigrants and natives in the US labor force, based on IPUMS Census microdata.

## üìä Project Overview

### Research Questions
1. **Occupational Downgrading**: Do immigrants face lower occupational scores compared to natives with similar education?
2. **Native Displacement**: Are natives experiencing economic downgrading due to immigration?
3. **Occupational Mobility**: How does time in the US affect immigrants' occupational outcomes?
4. **Educational Returns**: Are educational returns different for immigrants vs. natives?
5. **Cohort Effects**: Which migration cohorts achieve better outcomes?

### Dataset
- **Source**: IPUMS USA Census Microdata
- **Size**: 77.8M total observations ‚Üí 35.8M employed adults (after filtering)
- **Features**: 93 variables (demographics, migration status, education, employment)
- **Targets**:
  - `OCCSCORE` (Occupational prestige score: 0-80) - **Regression**
  - `CLASSWKR` (Class of worker: Self-employed/Private/Government) - **Classification**
- **Weights**: `PERWT` (person weights for population-representative estimates)

---

## üöÄ Quick Start

### Prerequisites
```bash
# Python 3.8+
pip install polars pandas numpy scikit-learn xgboost matplotlib seaborn torch
```

### GPU Setup (Optional but Recommended)
```bash
# For CUDA support (10-50x speedup on RTX 4060)
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

### Run the Pipeline
```bash
python main.py
```

---

## üìÅ Project Structure
```
project/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ usa_00002.dta              # Raw IPUMS data (77M rows)
‚îÇ   ‚îú‚îÄ‚îÄ ipums_data.parquet         # Extracted data
‚îÇ   ‚îú‚îÄ‚îÄ ipums_data_processed.parquet   # After missing value handling
‚îÇ   ‚îî‚îÄ‚îÄ ipums_data_ml_ready.parquet    # Final ML-ready data
‚îÇ
‚îú‚îÄ‚îÄ preprocessing/
‚îÇ   ‚îú‚îÄ‚îÄ extract.py                 # Step 1: Extract .dta to parquet
‚îÇ   ‚îú‚îÄ‚îÄ missing_values.py          # Step 2: Handle missing values
‚îÇ   ‚îú‚îÄ‚îÄ feature_engineering.py     # Step 3: Create features
‚îÇ   ‚îú‚îÄ‚îÄ preprocess_data_xgboost.py # Step 4: Train-test split
‚îÇ   ‚îî‚îÄ‚îÄ clip_outliers.py           # Utility: Outlier handling
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ xgboost_model.py          # XGBoost training (GPU-optimized)
‚îÇ   ‚îî‚îÄ‚îÄ metrics.py                # Reusable evaluation metrics
‚îÇ
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îú‚îÄ‚îÄ regression/               # OCCSCORE model outputs
‚îÇ   ‚îî‚îÄ‚îÄ classification/           # CLASSWKR model outputs
‚îÇ
‚îú‚îÄ‚îÄ main.py                       # Main training pipeline
‚îî‚îÄ‚îÄ README.md                     # This file
```

---

## üîß Pipeline Steps

### **Step 1: Data Extraction** (`extract.py`)
Converts raw IPUMS `.dta` file to efficient Parquet format.
```python
# Processes 77M rows in chunks to avoid memory issues
# Drops outcome variables that would leak information
# Output: ipums_data.parquet (35.8M employed adults)
```

**Key Operations**:
- ‚úÖ Filters to employed adults only (`empstat == 1`)
- ‚úÖ Drops answer keys and post-hiring variables
- ‚úÖ Chunked processing (500k rows at a time) for memory safety

---

### **Step 2: Missing Value Handling** (`missing_values.py`)
Handles structural vs. true missing data with domain knowledge.
```python
# Structural missingness: Migration variables (natives have no immigration year)
# True missingness: Random non-response (impute with median + indicator)
```

**Strategies**:
- **Migration Variables** (Structural):
  - `YRIMMIG`: Natives ‚Üí 0, Immigrants ‚Üí actual year
  - `YRNATUR`: Non-citizens/Natives ‚Üí 0, Naturalized ‚Üí actual year
  - Creates: `years_in_us` (exposure to US economy)

- **True Missing** (Random):
  - Numeric: Median imputation + `_missing` indicator
  - Categorical: Sentinel value (-1) + `_missing` indicator

**Output**: `ipums_data_processed.parquet`

---

### **Step 3: Feature Engineering** (`feature_engineering.py`)
Creates research-relevant features from raw variables.

#### **A. Education & Skills**
```python
# DEGFIELD ‚Üí is_stem (1 if Engineering/Science/Math, 0 otherwise)
STEM_CODES = [11, 13, 14, 21, 24, 25, 36, 37, 50, 51]

# SPEAKENG ‚Üí proficient_english (1 if speaks well/very well, 0 otherwise)
```

#### **B. Origin & Migration**
```python
# BPL (birthplace) ‚Üí origin_region (18 categories)
# Examples: Mexico, Canada, East_Asia, South_Asia, Western_Europe, etc.

# Derived: origin_development_level (6 economic groups)
# High_Income_Western, Latin_America, Developing_Asia, etc.
```

#### **C. Migration Block**
```python
# years_in_us: For immigrants (YEAR - YRIMMIG), For natives (AGE)
# age_at_arrival: AGE - years_in_us (immigrants only)
# is_naturalized: Binary flag from citizenship_status
```

#### **D. Family & Household**
```python
# family_burden: NCHILD + NSIBS
# is_married: Binary from MARST
# has_children: Binary from NCHILD > 0
# in_metro: Binary for urban vs rural
```

#### **E. Interaction Features** (for research questions)
```python
# immigrant_x_education: Tests differential returns to education
# immigrant_x_stem: Tests STEM credential discounting
# immigrant_x_english: Tests language proficiency effects
# tenure_x_education: Tests if returns grow with US experience
```

**Output**: `ipums_data_ml_ready.parquet` (65 features)

---

### **Step 4: Preprocessing for XGBoost** (`preprocess_data_xgboost.py`)

**Memory-Optimized Strategy**:
1. Load with Polars (3-5x more memory efficient than Pandas)
2. Split indices first (not data) to avoid duplication
3. Convert train/test to Pandas separately
4. Aggressive garbage collection
```python
X_train, X_test, y_train, y_test, w_train, w_test = preprocess_data_xgboost(
    data_path="ipums_data_ml_ready.parquet",
    target_column='occscore',
    weight_column='perwt',
    exclude_columns=['year', 'sample', 'classwkr', 'hwsei'],
    test_size=0.2,
    stratify_column='year',  # Ensures balanced census years
    verbose=True
)
```

**Key Features**:
- ‚úÖ Stratified split by `year` (prevents temporal bias)
- ‚úÖ Preserves sample weights (`perwt`)
- ‚úÖ One-hot encodes categorical variables
- ‚úÖ Removes constant columns
- ‚úÖ Memory: ~8GB peak (vs ~16GB for naive approach)

---

### **Step 5: Model Training** (`xgboost_model.py`)

#### **Model A: Regression (OCCSCORE)**
Predicts occupational prestige score (0-80).
```python
results_regression = train_xgboost_regressor(
    dtrain=dtrain_occ,  # Pre-made DMatrix (avoids double memory spike)
    dtest=dtest_occ,
    use_gpu=True,
    gpu_id=0,
    num_boost_round=1000,
    early_stopping_rounds=50,
    save_dir='./results/regression'
)
```

**Hyperparameters**:
- `max_depth`: 8 (tree depth)
- `eta`: 0.05 (learning rate, slower = better)
- `subsample`: 0.8 (row sampling)
- `colsample_bytree`: 0.8 (column sampling)
- `lambda`: 1.0 (L2 regularization)
- `alpha`: 0.1 (L1 regularization)

**Evaluation Metrics**:
- RMSE (Root Mean Squared Error)
- R¬≤ (Coefficient of Determination)
- Feature Importance (Gain)

#### **Model B: Classification (CLASSWKR)**
Predicts class of worker (Self-employed/Private/Government).
```python
results_classification = train_xgboost_classifier(
    dtrain=dtrain_cls,
    dtest=dtest_cls,
    use_gpu=True,
    params={'objective': 'multi:softprob', 'num_class': 3}
)
```

**Evaluation Metrics**:
- Multi-class Log Loss
- Accuracy, Precision, Recall, F1-Score
- Confusion Matrix (weighted by `perwt`)

---

## üìà Results & Outputs

### **Saved Files**
```
results/
‚îú‚îÄ‚îÄ regression/
‚îÇ   ‚îú‚îÄ‚îÄ xgboost_occscore_model.json       # Trained model
‚îÇ   ‚îú‚îÄ‚îÄ feature_importance_occscore.csv   # Top features
‚îÇ   ‚îú‚îÄ‚îÄ regression_diagnostics.png        # Actual vs Predicted
‚îÇ   ‚îî‚îÄ‚îÄ learning_curves.png               # Train/test RMSE over iterations
‚îÇ
‚îî‚îÄ‚îÄ classification/
    ‚îú‚îÄ‚îÄ xgboost_classwkr_model.json
    ‚îú‚îÄ‚îÄ feature_importance_classification.csv
    ‚îú‚îÄ‚îÄ confusion_matrix.png
    ‚îî‚îÄ‚îÄ learning_curves.png
```

### **Expected Performance** (on test set)
- **Regression**: R¬≤ ‚âà 0.45-0.55 (reasonable for social science)
- **Classification**: Accuracy ‚âà 70-80% (depends on class imbalance)

### **Top Features** (typically)
1. `educ` / `educd` (Education level)
2. `age` (Experience proxy)
3. `years_in_us` (Time in US)
4. `is_stem` (STEM degree)
5. `origin_development_level` (Origin country economics)
6. `immigrant_x_education` (Differential returns)
7. `proficient_english` (Language skills)

---

## üß† Key Design Decisions

### **1. Why XGBoost?**
- ‚úÖ Handles non-linear relationships (education √ó immigration status)
- ‚úÖ Native support for sample weights (`perwt`)
- ‚úÖ Feature importance for interpretability
- ‚úÖ GPU acceleration (10-50x speedup)
- ‚úÖ Robust to outliers and missing values

### **2. Why Not Impute Migration Variables with Median?**
Migration variables have **structural missingness**:
- Natives don't have `YRIMMIG` because they were never immigrants
- Imputing median would assign nonsensical values
- Solution: Create domain-specific features (`years_in_us`, `is_naturalized`)

### **3. Why Use Sample Weights?**
IPUMS data uses complex sampling:
- Not all people have equal probability of selection
- `PERWT` adjusts for sampling bias
- Without weights: Model optimized for survey respondents
- With weights: Model optimized for US population

### **4. Why Stratify by Year?**
Census years have different:
- Economic conditions (2008 recession vs. 2019 boom)
- Population demographics
- Survey methodology
- Stratification ensures balanced representation in train/test

### **5. Memory Optimization**
35M rows √ó 65 features = **huge memory footprint**:
- Polars for initial processing (2-3x less memory than Pandas)
- Index-based splitting (no data duplication)
- Pre-create DMatrix objects (avoid double allocation)
- Aggressive garbage collection
- Result: Runs on 16GB RAM (vs. requiring 32GB+)

---

## üî¨ Research Implications

### **Testing Your Hypotheses**

#### **Q1: Occupational Downgrading?**
```python
# Compare OCCSCORE for immigrants vs. natives with same education
# Check feature importance of 'is_immigrant'
# Look at residuals: Do immigrants have lower scores than predicted?
```

#### **Q2: Native Displacement?**
```python
# Examine 'immigrant_x_education' coefficient
# If negative: Natives with high education benefit less in high-immigration areas
```

#### **Q3: Occupational Mobility?**
```python
# Plot OCCSCORE vs. years_in_us
# Check if 'tenure_x_education' has positive effect
# Expected: Scores improve with time, but plateau
```

#### **Q4: Educational Returns?**
```python
# Compare SHAP values for 'educ' for immigrants vs. natives
# If 'immigrant_x_education' is negative: Credential discounting exists
```

#### **Q5: Cohort Effects?**
```python
# Group by 'origin_region' or 'origin_development_level'
# Check if East_Asia has higher OCCSCORE than Latin_America (controlling for education)
```

---

## üõ†Ô∏è Advanced Usage

### **Hyperparameter Tuning**
```python
# Use XGBoost's built-in CV
import xgboost as xgb

params_grid = {
    'max_depth': [6, 8, 10],
    'eta': [0.01, 0.05, 0.1],
    'subsample': [0.7, 0.8, 0.9]
}

# Run grid search (warning: computationally expensive!)
```

### **SHAP Analysis** (for interpretability)
```python
import shap

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# Plot feature importance
shap.summary_plot(shap_values, X_test)

# Individual prediction explanation
shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0])
```

### **Handling Class Imbalance** (for CLASSWKR)
```python
# If one class dominates (e.g., 80% private sector):
params = {
    'scale_pos_weight': n_negative / n_positive,  # For binary
    # OR
    'max_delta_step': 1  # For multiclass, limits prediction changes
}
```

---

## üìö References

- **IPUMS USA**: https://usa.ipums.org/
- **XGBoost Documentation**: https://xgboost.readthedocs.io/
- **Chen & Guestrin (2016)**: "XGBoost: A Scalable Tree Boosting System"

---

## ü§ù Contributing

For researchers extending this work:
1. **Add new features**: Edit `feature_engineering.py`
2. **Try different models**: Create new file in `models/` (metrics.py is reusable)
3. **Analyze subgroups**: Filter data in preprocessing step
4. **Alternative targets**: Change `target_column` in main.py

---

## üìù Citation

If you use this pipeline in your research:
```bibtex
@software{ipums_immigration_study,
  title={IPUMS Immigration Occupational Outcomes Prediction},
  author={Your Name},
  year={2024},
  note={XGBoost pipeline for analyzing immigrant vs. native labor market outcomes}
}
```

---

## ‚ö†Ô∏è Troubleshooting

### **Out of Memory Error**
```python
# Use chunked processing
from preprocessing.preprocess_data_xgboost import preprocess_data_xgboost_chunked

X_train, X_test, y_train, y_test, w_train, w_test = preprocess_data_xgboost_chunked(
    data_path="ipums_data_ml_ready.parquet",
    target_column='occscore',
    chunk_size=500000  # Process 500k rows at a time
)
```

### **GPU Not Detected**
```bash
# Check CUDA installation
nvidia-smi

# Install PyTorch with CUDA
pip install torch --index-url https://download.pytorch.org/whl/cu118

# Fallback to CPU (10x slower but works)
results = train_xgboost_regressor(dtrain, dtest, use_gpu=False)
```

### **Slow Training**
```python
# Reduce data size (for prototyping)
df = df.sample(frac=0.1, random_state=42)  # Use 10% of data

# OR increase learning rate (less accurate)
params = {'eta': 0.1}  # Default is 0.05
```

---

## üìß Contact

For questions about the pipeline: [Your Email]
For questions about IPUMS data: https://usa.ipums.org/usa/

---

**License**: MIT (or your preferred license)

**Last Updated**: December 2024